{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13080603-dc97-4359-8af5-d045e6e44af3",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9583f340-00bd-4667-b90d-042213c5f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a document to be used for retrieval\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file = \"~/Desktop/langchain_concepts/hdfc_policy_doc.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d49f418-9b5b-4141-85c2-2b7566c3fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "# will load each page as a document, with lazy loading\n",
    "document_pages = []\n",
    "async for page in pdf_loader.alazy_load():\n",
    "    document_pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e82ad01-6d4c-4e48-9157-2cd0bcf8a429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b9dafa-4cd0-405c-941b-a3fc4d278bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-03-10T17:06:03+05:30', 'author': 'harinis', 'moddate': '2025-03-10T17:06:03+05:30', 'source': '/Users/sarmehta/Desktop/langchain_concepts/hdfc_policy_doc.pdf', 'total_pages': 48, 'page': 0, 'page_label': '1'}, page_content='Page 1 of 48 \\n \\n \\nPart A \\n(Welcome Letter) \\n<<Date>> \\n<<Policyholder’s Name>>  \\n<<Policyholder’s Address>> \\n<<Policyholder’s Contact Number>> \\n \\nSub: Your Policy no. <<>>-HDFC Life Sampoorn Nivesh Plus \\n \\nDear <<Policyholder’s Name>>,  \\n \\nWe are glad to inform you that your proposal has been accepted and the HDFC Life Sampoorn  Nivesh Plus \\nPolicy (“Policy”) being this Policy, has been issued. We have made every effort to design your Policy in a \\nsimple format. We have highlighted items of importance so that you may recognize them easily. \\n \\nPolicy document: \\nAs evidence of the insurance contract between HDFC Life Insurance Company Limited and you, the Policy is \\nenclosed herewith. Please preserve this document safely and inform your nominees about the same. A copy of \\nyour proposal form and other relevant documents submitted by you is also enclosed  for your information and \\nrecord.  \\n \\nCancellation in the Free-Look Period: \\nIn case you are not agreeable to any of the terms and conditions stated in th e Policy, you have the option to \\nreturn the policy to us for cancellation stating the reasons thereof, within 30 days from the date of receipt of the \\nPolicy whether rece ived electronically or otherwise . On receipt of your letter along with the original \\npolicy(original Policy Document is not required for policies in dematerialised form or where policy is issued \\nonly in electronic form), we shall arrange to refund  the value of Units allocated to you on the date of receipt of \\nrequest plus the unallocated part of the premium plus charges levied by cancellation of Units, subject to \\ndeduction of the proportionate risk premium for the period on cover expenses incurred on medical examination \\nof the proposer and stamp duty charges (if any). \\n \\nContact us: \\nFor any assistance with your policy or services, please call us at 022-68446530 (Mon to Sat – 10 AM to 7 PM \\nIST) or email us at service@hdfclife.com. Please quote your Policy number in all correspondence. Our postal \\naddress for correspondence is as specified below. You may reach out to your Certified Financial Consultant \\n(Insurance Agent) who assisted you with this policy. \\n \\nAgent details: \\nName:<<HDFC BANK, Address: HDFC Bank Ltd, Sandoz House, 2nd Floor, Shiv Sagar Estate, Dr. Annie \\nBesant Road, Worli, Mumbai, Maharashtra -400018 Agent No.:  01546366 License No.:  CA0010 Contact \\nDetails: XXXXXXX>> \\n \\nTo contact us in case of any grievance, please Click here,you may also refer to Part G. \\n \\nYours sincerely, \\n \\n \\nAuthorised Signatory')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49235168-88e2-44e4-8361-87d1acd55b46",
   "metadata": {},
   "source": [
    "# Chat Model/ LLM Model\n",
    "Here if we have the resources, we can use a local LLM chat model\n",
    "or otherwise use API for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4fcd928-2e81-45d2-9326-c7a36ab9656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat model for making the retrieval process conversational also\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc99f6d-4fc8-4254-acd2-82b055075a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nllm_model = HuggingFacePipeline.from_model_id(\\n    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\\n    task=\"text-generation\",\\n    pipeline_kwargs=dict(\\n        max_new_tokens=512,\\n        do_sample=False,\\n        repetition_penalty=1.03,\\n    ),\\n)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this can be used in local if we have memory, otherwise \n",
    "#through API\n",
    "\"\"\"\n",
    "llm_model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba0b4e8-ad3d-4401-819f-58f105659ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_model = ChatHuggingFace(llm=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5ae692c-3980-4143-aa16-07ac481ccde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case if not loading model on RAM, can use LLM API as well\n",
    "import os\n",
    "from api_keys import GROQ_API_KEY\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "llm_model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02e897-f774-45d5-8727-bca1cd09b6a8",
   "metadata": {},
   "source": [
    "# Text Splitter\n",
    "creating text splitter for our document which will break,\n",
    "for document either we can go with Recursive Splitter or also Semantic splitter\n",
    "if the document has content which are more of semantic reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b03fb137-fde5-459c-9635-ee3928cab946",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 300\n",
    "OVERLAP = 30\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = CHUNK_SIZE,\n",
    "                                               chunk_overlap = OVERLAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ed5d5-f94a-4be7-b888-06e467f16776",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a01f293-4d6b-4715-81e5-06089986a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model for indexing\n",
    "EMBED_DIMS = 768\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embed_model = HuggingFaceEmbeddings(model_name = \"thenlper/gte-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31018450-e787-41c6-a57c-ff414830d016",
   "metadata": {},
   "source": [
    "# Vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0008578d-4aad-455e-8e57-6bef8cb67cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss index to be used as vector store \n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(EMBED_DIMS)\n",
    "vector_store = FAISS(embedding_function = embed_model,\n",
    "                    index = faiss_index,\n",
    "                    docstore = InMemoryDocstore(),\n",
    "                    index_to_docstore_id = {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecf56c-5c06-4f62-bb16-570c0065a53e",
   "metadata": {},
   "source": [
    "Here for providing the context to the LLM for retrieval, we have two options,\n",
    "\n",
    "1. Chunk Context -- In this, if we just store the text spliited (chunked) documents in vector store, the vector store retriever would just retrieve the chunk for LLM would have this chunk as the context. It can be used if the chunk size is big enough, and we just need to generate an answer to the query to the user.\n",
    "\n",
    "2. Document context -- How ever, if we need more context, and also need give the user the related document from which the answer is generated, we need to keep the mapping of the vector retrieved chunk to the original document. This can be done by using `ParentDocumentRetriever` in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31399251-535f-447e-9755-2dfbb75bd0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the chunked documents to the vector store\n",
    "all_splits = text_splitter.split_documents(document_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b517ebc-7045-415c-aa64-5130ad57e7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-03-10T17:06:03+05:30', 'author': 'harinis', 'moddate': '2025-03-10T17:06:03+05:30', 'source': '/Users/sarmehta/Desktop/langchain_concepts/hdfc_policy_doc.pdf', 'total_pages': 48, 'page': 0, 'page_label': '1'}, page_content='Details: XXXXXXX>> \\n \\nTo contact us in case of any grievance, please Click here,you may also refer to Part G. \\n \\nYours sincerely, \\n \\n \\nAuthorised Signatory')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "630a9d58-981a-41ad-9b43-ab6513666203",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd7842-c211-4303-8099-e039083a55cc",
   "metadata": {},
   "source": [
    "# Conversational Graph\n",
    "We will make the RAG application in the LangGraph so as to follow the conversational flow\n",
    "\n",
    "START -> retrieve_node(node1) --> generate_node(node2) --> END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e59c47e-980e-4454-a34e-02197903df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining state class\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72e755df-8ed7-4b3a-b64c-56e087e448a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47d52831-4ce3-454e-bdfe-b246e8046b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27a6a37c-c05e-4a9a-810c-28bb2b02b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node 1 for retriving context related to the question\n",
    "def retrieval_node(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3079d7bf-258f-406d-8d78-e3b35c590e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node 2 for running the LLm with the context\n",
    "# for this we will have to create a prompt first which \n",
    "# will contain the instruction, question, retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a9a908b-155e-42f8-8552-7d1e508fdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05f87937-3f20-4748-8d05-41fe20416006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarmehta/miniconda3/envs/langchain/lib/python3.11/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f9408af-f33f-4427-b2d0-a050a1a65594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(input_variables=['context', 'question'],\n",
    "                            messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2866af6-6e60-40ea-91d0-06b4e868119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325d9cf-e62a-410b-9958-dbebe2dbf97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
