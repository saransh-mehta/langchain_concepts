{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b0ace2-fb3c-4100-b407-9de9641cd49b",
   "metadata": {},
   "source": [
    "# Langchain Embeddings\n",
    "\n",
    "LangChain provides a universal interface for working with them, providing standard methods for common operations. This common interface simplifies interaction with various embedding providers through two central methods:\n",
    "\n",
    "embed_documents: For embedding multiple texts (documents)\n",
    "embed_query: For embedding a single text (query)\n",
    "\n",
    "# Lanchain - Hugggingface integration\n",
    "\n",
    "Langchain supports huggingface, through which we can use models provided by huggingface\n",
    "into langchain, this can be useful when we don't want to directly use LLM API \n",
    "or when we just want to use an embedding model to create vectors for our querying system\n",
    "\n",
    "Need to install the huggingface langchain package\n",
    "\n",
    "pip install langchain-huggingface\n",
    "\n",
    "Langchain has multiple classes to use huggingface API, embedding model, \n",
    "huggingface pipeline etc\n",
    "\n",
    "# Embedding model\n",
    "\n",
    "Some of the best embedding models to use \n",
    "\n",
    "### Jina embedding - `jina_embeddings-v3` \n",
    "its XLM-RoBERTa based multi-task text embedding model with multilingual embedding, 8192 tokens\n",
    "Jina model is trained using `LoRA adapters` training method, which have been fine tuned for multiple tasks\n",
    "\n",
    "retrieval.query: Used for query embeddings in asymmetric retrieval tasks\n",
    "retrieval.passage: Used for passage embeddings in asymmetric retrieval tasks\n",
    "separation: Used for embeddings in clustering and re-ranking applications\n",
    "classification: Used for embeddings in classification tasks\n",
    "text-matching: Used for embeddings in tasks that quantify similarity between two texts, such as STS or symmetric retrieval tasks\n",
    "\n",
    "can pass these any of these task in `task` parameter to encode function with that specific adapter. Can also use it without specifying \n",
    "any task specifc.\n",
    "\n",
    "By default, the model supports a maximum sequence length of 8192 tokens. However, if you want to truncate your input texts to a shorter length, you can pass the max_length parameter to the encode function. Matryoshka Embeddings: Supports flexible embedding sizes (32, 64, 128, 256, 512, 768, 1024), allowing for truncating embeddings to fit your application.\n",
    "\n",
    "### GTE embedding - (GTE large, base, small)  `thenlper/gte-base`\n",
    "\n",
    "General Text Embeddings (GTE) model from Alibaba. This is BERT based model\n",
    "These are more of a general purpose embedding, celebrated for its flexibility across a broad spectrum of NLP tasks.\n",
    "This is due to large variety of data they've used to train. Training in two steps\n",
    "\n",
    "IMPORTANT - They are good, because they've used CONTRANSTIVE LEARNING LOSS  to train (selecting pairs of query, for a query q, a \n",
    "positive sample is picked (related), and negative samples are also mined, trained in triplet kind of setup)\n",
    "This loss has been used for both stages\n",
    "\n",
    "Stage 1 - unsupervised training - web scrapped data in text pairs\n",
    "Stage 2 - good quality human annotated data in text pairs \n",
    "\n",
    "\n",
    "### Sentence transformers embeddings\n",
    "\n",
    "`all-mpnet\n",
    "\n",
    "Can use `HuggingFaceEmbeddings` class. It also supports model_kwargs\n",
    "We can also use the AutoModel from huggingface to vectorize, in case the langchain-huggingface integration isn't maintained or has errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6518d3b2-8dac-4037-97ee-424b6883f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Let me check out text splitting for document retrievals with langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a24e834-8628-43ab-a8c9-c3f3a9cd2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from  langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "239ad44a-04f4-4a9d-b5fc-9d84107da264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the huggingface-langchain integration is based on sentence-transformers implementation\n",
    "# it will throw errors if we try to use non sentence transformers models, like Jinaai, which \n",
    "# has its own some custom code. Hence we will use the Jina-langchain community \n",
    "from langchain_community.embeddings import JinaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "566a819a-ed37-4e2f-b1ed-4b7d36ff4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "JINA_API_KEY = \"jina_bba3b217b6cb41e19437d71fc7d2492fBLmMxyrH9ZsByHz1I1ipKf8ScnuZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d46515a5-be7e-4383-bb19-50ae41e5eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = JinaEmbeddings(jina_api_key=JINA_API_KEY, model_name = \"jina-embeddings-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38d503cd-43b7-4d12-b7c8-4e54a52945bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_doc = embed_model.embed_documents([sample_text])\n",
    "embed_query = embed_model.embed_query(sample_text)\n",
    "len(embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb96f28b-3cc9-44a0-85a7-dac55cede25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gte embedding\n",
    "embed_model = HuggingFaceEmbeddings(model_name = \"thenlper/gte-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1baea58-582c-41d3-8663-49835295284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_query = embed_model.embed_query(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46dd6da2-ce70-428e-ace3-18a18cfbfcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28319fde-3bde-48a0-a892-6fe6dd561594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbbd462e-ca60-40da-827a-e5738f75a213",
   "metadata": {},
   "source": [
    "# Vector Indexing with FAISS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain]",
   "language": "python",
   "name": "conda-env-langchain-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
